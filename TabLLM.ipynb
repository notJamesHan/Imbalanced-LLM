{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "19sey1k-7eEv39fr6WFJhudMl9ick2ztJ",
      "authorship_tag": "ABX9TyNqT5eE8lkAJcV9vL+O4qvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notJamesHan/Imbalanced-LLM/blob/enc%2F3-test-tabllm-modified-code/TabLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hwlPRQ9EwiT",
        "outputId": "f4317a9d-9870-4987-873b-3113e7961428"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run only one time.\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/TabLLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjRu2gLEJka6",
        "outputId": "3b04a559-62aa-46b4-8ab0-84a0ccf0e8fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/TabLLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the main requirements\n",
        "!pip install -q -U git+https://github.com/bigscience-workshop/promptsource.git\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U --use-deprecated=legacy-resolver -r ./requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4-wAgP7IPVT",
        "outputId": "33c0b2fe-acce-4128-bdd7-d31ccbfebf4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for promptsource (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.6 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Hugging Face Cache\n",
        "!export HF_HOME=~/.cache/huggingface"
      ],
      "metadata": {
        "id": "zAvnfnq0iuno"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ******[WARNING]****** REMOVVING ALL PRE-EXISTING RESULTS\n",
        "%rm -r exp_out"
      ],
      "metadata": {
        "id": "Ire5F0VUPMFb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "# !chmod +rwx /content/drive/MyDrive/Colab\\ Notebooks/TabLLM/bin/test.sh\n",
        "# !bash ./bin/test.sh\n",
        "!chmod +rwx /content/drive/MyDrive/Colab\\ Notebooks/TabLLM/bin/few-shot-pretrained-100k.sh\n",
        "!bash ./bin/few-shot-pretrained-100k.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRhs-6gAGZQr",
        "outputId": "83ee3506-f8ee-4b6b-ea67-205ae3f8d57f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start experiment t03b_heart_numshot4_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t03b_heart_numshot4_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"bigscience/T0_3B\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t03b_ia3_finish.pt\",\n",
            "    \"dataset\": \"heart\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 4,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"bf16-mixed\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 30,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t03b_heart_numshot4_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t03b_heart_numshot4_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for heart heart_disease, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "Training\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/TabLLM/src/pl_train.py\", line 79, in <module>\n",
            "    main(config)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/TabLLM/src/pl_train.py\", line 42, in main\n",
            "    trainer = Trainer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/argparse.py\", line 70, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 401, in __init__\n",
            "    self._accelerator_connector = _AcceleratorConnector(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 147, in __init__\n",
            "    self._accelerator_flag = self._choose_gpu_accelerator_backend()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 367, in _choose_gpu_accelerator_backend\n",
            "    raise MisconfigurationException(\"No supported gpu backend found!\")\n",
            "lightning_fabric.utilities.exceptions.MisconfigurationException: No supported gpu backend found!\n"
          ]
        }
      ]
    }
  ]
}
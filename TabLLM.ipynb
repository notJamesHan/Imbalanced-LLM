{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notJamesHan/Imbalanced-LLM/blob/enc%2F3-test-tabllm-modified-code/TabLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hwlPRQ9EwiT",
        "outputId": "09c2b8f4-bde2-4176-c8a4-a3a463678a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjRu2gLEJka6",
        "outputId": "d1dfdb38-9845-4fad-cf9a-63c27eb599d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/TabLLM\n"
          ]
        }
      ],
      "source": [
        "# Run only one time.\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/TabLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4-wAgP7IPVT",
        "outputId": "df664e49-979a-444d-87e3-e0dc2c9396fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for promptsource (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.6 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install the main requirements\n",
        "!pip install -q -U git+https://github.com/bigscience-workshop/promptsource.git\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U --use-deprecated=legacy-resolver -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zAvnfnq0iuno"
      },
      "outputs": [],
      "source": [
        "# Setting Hugging Face Cache\n",
        "!export HF_HOME=~/.cache/huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ire5F0VUPMFb"
      },
      "outputs": [],
      "source": [
        "# ******[WARNING]****** REMOVVING ALL PRE-EXISTING RESULTS\n",
        "%rm -r exp_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRhs-6gAGZQr",
        "outputId": "ba6c1754-c130-42fa-b4ef-ceca545af53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start experiment t5_heart_numshot4_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_heart_numshot4_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"heart\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 4,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 30,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_heart_numshot4_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for heart heart_disease, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_heart_numshot4_seed42_ia3_pretrained100k/log\n",
            "2024-04-24 15:50:36.719038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 15:50:36.719084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 15:50:36.720443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 15:50:37.854513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 4\n",
            "Eval size 184\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 1/1 [00:00<00:00,  2.99it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/23 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/23 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  87% 20/23 [00:01<00:00, 18.03it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 23/23 [00:01<00:00, 18.68it/s]\u001b[A\n",
            "{\"AUC\": 0.8408793820558527, \"PR\": 0.8319278134222968, \"micro_f1\": 0.657608695652174, \"macro_f1\": 0.6476060191518468, \"accuracy\": 0.657608695652174, \"num\": 184, \"num_steps\": -1, \"score_gt\": 0.32680677331012226, \"score_cand\": 0.45429793648097827}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.8408793820558527] at step 30.\n",
            "\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.03s/it, v_num=0]`Trainer.fit` stopped: `max_steps=30` reached.\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.03s/it, v_num=0]\n",
            "Start experiment t5_heart_numshot4_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_heart_numshot4_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"heart\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 4,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 30,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_heart_numshot4_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for heart heart_disease, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_heart_numshot4_seed1024_ia3_pretrained100k/log\n",
            "2024-04-24 15:51:04.015274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 15:51:04.015327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 15:51:04.016580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 15:51:05.105854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 4\n",
            "Eval size 184\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 1/1 [00:00<00:00,  3.02it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/23 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/23 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  87% 20/23 [00:01<00:00, 19.88it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 23/23 [00:01<00:00, 20.67it/s]\u001b[A\n",
            "{\"AUC\": 0.8748012232415902, \"PR\": 0.9022753150520015, \"micro_f1\": 0.7445652173913043, \"macro_f1\": 0.7441949892034194, \"accuracy\": 0.7445652173913043, \"num\": 184, \"num_steps\": -1, \"score_gt\": 0.2690578958262568, \"score_cand\": 0.6128062372622283}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.8748012232415902] at step 30.\n",
            "\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.03s/it, v_num=0]`Trainer.fit` stopped: `max_steps=30` reached.\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.04s/it, v_num=0]\n",
            "Start experiment t5_heart_numshot4_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_heart_numshot4_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"heart\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 4,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 30,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_heart_numshot4_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for heart heart_disease, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_heart_numshot4_seed0_ia3_pretrained100k/log\n",
            "Flattening the indices: 100% 734/734 [00:00<00:00, 37375.94 examples/s]\n",
            "Via sampling with replacement old label distribution {0: 320, 1: 414} to new {0: 2, 1: 2}\n",
            "2024-04-24 15:51:31.406893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 15:51:31.406948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 15:51:31.408225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 15:51:32.486761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Flattening the indices: 100% 184/184 [00:00<00:00, 11100.03 examples/s]\n",
            "Train size 4\n",
            "Eval size 184\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 1/1 [00:00<00:00,  2.81it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/23 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/23 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  87% 20/23 [00:01<00:00, 19.99it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 23/23 [00:01<00:00, 20.73it/s]\u001b[A\n",
            "{\"AUC\": 0.8342198581560284, \"PR\": 0.8033168193978506, \"micro_f1\": 0.766304347826087, \"macro_f1\": 0.7657438934122872, \"accuracy\": 0.7663043478260869, \"num\": 184, \"num_steps\": -1, \"score_gt\": 0.28171904190726904, \"score_cand\": 0.47650345512058423}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.8342198581560284] at step 30.\n",
            "\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.04s/it, v_num=0]`Trainer.fit` stopped: `max_steps=30` reached.\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.05s/it, v_num=0]\n",
            "Start experiment t5_heart_numshot4_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_heart_numshot4_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"heart\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 4,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 30,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_heart_numshot4_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for heart heart_disease, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_heart_numshot4_seed1_ia3_pretrained100k/log\n",
            "Flattening the indices: 100% 734/734 [00:00<00:00, 17768.06 examples/s]\n",
            "Via sampling with replacement old label distribution {0: 334, 1: 400} to new {0: 2, 1: 2}\n",
            "2024-04-24 15:51:59.209524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 15:51:59.209575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 15:51:59.210797: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 15:52:00.362691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Flattening the indices: 100% 184/184 [00:00<00:00, 10284.13 examples/s]\n",
            "Train size 4\n",
            "Eval size 184\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 1/1 [00:00<00:00,  1.74it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/23 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/23 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  87% 20/23 [00:01<00:00, 12.70it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 23/23 [00:01<00:00, 13.65it/s]\u001b[A\n",
            "{\"AUC\": 0.8651924951267057, \"PR\": 0.8932604994277422, \"micro_f1\": 0.7771739130434783, \"macro_f1\": 0.774772354082699, \"accuracy\": 0.7771739130434783, \"num\": 184, \"num_steps\": -1, \"score_gt\": 0.26587312117866846, \"score_cand\": 0.6079347029976223}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.8651924951267057] at step 30.\n",
            "\n",
            "Epoch 29: 100% 1/1 [00:03<00:00,  3.15s/it, v_num=0]`Trainer.fit` stopped: `max_steps=30` reached.\n",
            "Epoch 29: 100% 1/1 [00:03<00:00,  3.15s/it, v_num=0]\n",
            "Start experiment t5_heart_numshot4_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_heart_numshot4_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"heart\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 4,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 30,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_heart_numshot4_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for heart heart_disease, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_heart_numshot4_seed32_ia3_pretrained100k/log\n",
            "Flattening the indices: 100% 734/734 [00:00<00:00, 31779.62 examples/s]\n",
            "Via sampling with replacement old label distribution {0: 328, 1: 406} to new {0: 2, 1: 2}\n",
            "2024-04-24 15:52:29.475748: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 15:52:29.475812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 15:52:29.477543: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 15:52:31.028993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Flattening the indices: 100% 184/184 [00:00<00:00, 5774.73 examples/s]\n",
            "Train size 4\n",
            "Eval size 184\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 1/1 [00:00<00:00,  2.04it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/23 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/23 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  87% 20/23 [00:01<00:00, 18.88it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 23/23 [00:01<00:00, 19.64it/s]\u001b[A\n",
            "{\"AUC\": 0.8552725968436155, \"PR\": 0.8729922293593524, \"micro_f1\": 0.7717391304347826, \"macro_f1\": 0.7704099821746881, \"accuracy\": 0.7717391304347826, \"num\": 184, \"num_steps\": -1, \"score_gt\": 0.28032518469769024, \"score_cand\": 0.47996122940726904}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.8552725968436155] at step 30.\n",
            "\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.48s/it, v_num=0]`Trainer.fit` stopped: `max_steps=30` reached.\n",
            "Epoch 29: 100% 1/1 [00:02<00:00,  2.48s/it, v_num=0]\n"
          ]
        }
      ],
      "source": [
        "# Run Training\n",
        "# !chmod +rwx /content/drive/MyDrive/Colab\\ Notebooks/TabLLM/bin/test.sh\n",
        "# !bash ./bin/test.sh\n",
        "!chmod +rwx /content/drive/MyDrive/Colab\\ Notebooks/TabLLM/bin/few-shot-pretrained-100k.sh\n",
        "!bash ./bin/few-shot-pretrained-100k.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Results\n",
        "!python src/scripts/get_result_table.py -e t5_\\* -d heart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv1o8yOSzn91",
        "outputId": "6fd2fab8-209f-454a-b7a6-24bb3861a4b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Find 5 experiments fit into t5_*\n",
            "heart: 85.41 (1.50)\n",
            "Save result to exp_out/summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRzxFs5bUXM1"
      },
      "outputs": [],
      "source": [
        "!dmesg"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notJamesHan/Imbalanced-LLM/blob/main/TabLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hwlPRQ9EwiT",
        "outputId": "d06dcc0f-9755-43dd-ec1f-1948241c1cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjRu2gLEJka6",
        "outputId": "485c25a1-ec8d-487f-d150-0ba878870ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/TabLLM\n"
          ]
        }
      ],
      "source": [
        "# Run only one time.\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/TabLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4-wAgP7IPVT",
        "outputId": "2fed19d4-a5d2-4ae1-ebec-b81740226fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for promptsource (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.6 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install the main requirements\n",
        "!pip install -q -U git+https://github.com/bigscience-workshop/promptsource.git\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U --use-deprecated=legacy-resolver -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAvnfnq0iuno"
      },
      "outputs": [],
      "source": [
        "# Setting Hugging Face Cache\n",
        "!export HF_HOME=~/.cache/huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ire5F0VUPMFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10d6dc9-42d9-4c14-8b89-af0867207edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'exp_out': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# ******[WARNING]****** REMOVVING ALL PRE-EXISTING RESULTS\n",
        "#%rm -r exp_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRhs-6gAGZQr",
        "outputId": "9183a98e-e0fa-4aa8-f439-b06d2c47f299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot8_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot8_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 4, 1: 4}\n",
            "2024-04-26 19:30:31.208129: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:30:31.208203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:30:31.314055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:30:32.581487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 8\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 2/2 [00:00<00:00,  2.87it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:08, 12.79it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 14.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 15.80it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 16.71it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 16.86it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.24it/s]\u001b[A\n",
            "{\"AUC\": 0.6102993655828708, \"PR\": 0.07793900015248241, \"micro_f1\": 0.4892367906066536, \"macro_f1\": 0.37302381993315187, \"accuracy\": 0.4892367906066536, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.33104775983060175, \"score_cand\": 0.3768020301415729}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6102993655828708] at step 60.\n",
            "\n",
            "Epoch 29: 100% 2/2 [00:09<00:00,  4.60s/it, v_num=0]`Trainer.fit` stopped: `max_steps=60` reached.\n",
            "Epoch 29: 100% 2/2 [00:09<00:00,  4.60s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot8_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot8_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 8,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 60,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot8_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot8_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 4, 1: 4}\n",
            "2024-04-26 19:31:16.026125: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:31:16.026185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:31:16.027707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:31:17.156648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 8\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 2/2 [00:00<00:00,  4.11it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.27it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.71it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.59it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.07it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.12it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.63it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.62it/s]\u001b[A\n",
            "{\"AUC\": 0.5761999486652978, \"PR\": 0.07047422397333991, \"micro_f1\": 0.2847358121330724, \"macro_f1\": 0.2516150618226332, \"accuracy\": 0.2847358121330724, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.34468416038557975, \"score_cand\": 0.35690038871391877}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.5761999486652978] at step 60.\n",
            "\n",
            "Epoch 29: 100% 2/2 [00:08<00:00,  4.17s/it, v_num=0]`Trainer.fit` stopped: `max_steps=60` reached.\n",
            "Epoch 29: 100% 2/2 [00:08<00:00,  4.17s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot8_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot8_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 8,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 60,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot8_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot8_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 4, 1: 4}\n",
            "2024-04-26 19:31:59.596738: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:31:59.596797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:31:59.598713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:32:00.858581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 8\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 2/2 [00:00<00:00,  4.06it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 15.49it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 14.30it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:02, 13.45it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.24it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 14.65it/s]\u001b[A\n",
            "{\"AUC\": 0.5199465376782078, \"PR\": 0.0780520007590631, \"micro_f1\": 0.4481409001956947, \"macro_f1\": 0.3347091412742382, \"accuracy\": 0.4481409001956947, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.370813418274523, \"score_cand\": 0.35920121105216485}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.5199465376782078] at step 60.\n",
            "\n",
            "Epoch 29: 100% 2/2 [00:09<00:00,  4.90s/it, v_num=0]`Trainer.fit` stopped: `max_steps=60` reached.\n",
            "Epoch 29: 100% 2/2 [00:09<00:00,  4.90s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot8_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot8_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 8,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 60,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot8_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot8_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 4, 1: 4}\n",
            "2024-04-26 19:32:43.382673: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:32:43.382732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:32:43.384239: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:32:44.489743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 8\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 2/2 [00:00<00:00,  3.76it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:10, 10.49it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:07, 11.13it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:05<00:05, 11.65it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:03, 12.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:02, 13.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.40it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 14.81it/s]\u001b[A\n",
            "{\"AUC\": 0.6506370277975766, \"PR\": 0.07816740043994819, \"micro_f1\": 0.42857142857142855, \"macro_f1\": 0.34237712432127493, \"accuracy\": 0.42857142857142855, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.35325583739756605, \"score_cand\": 0.34442413389799414}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6506370277975766] at step 60.\n",
            "\n",
            "Epoch 29: 100% 2/2 [00:09<00:00,  4.89s/it, v_num=0]`Trainer.fit` stopped: `max_steps=60` reached.\n",
            "Epoch 29: 100% 2/2 [00:09<00:00,  4.89s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot8_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot8_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 8,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 60,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot8_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot8_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 4, 1: 4}\n",
            "2024-04-26 19:33:28.083932: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:33:28.084002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:33:28.086043: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:33:29.437795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 8\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 29: 100% 2/2 [00:00<00:00,  3.58it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 15.00it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.92it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.21it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.38it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.83it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 18.19it/s]\u001b[A\n",
            "{\"AUC\": 0.664669480612391, \"PR\": 0.08477660319466429, \"micro_f1\": 0.46966731898238745, \"macro_f1\": 0.3803566234870349, \"accuracy\": 0.46966731898238745, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.34206717354910715, \"score_cand\": 0.3598667450846991}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.664669480612391] at step 60.\n",
            "\n",
            "Epoch 29: 100% 2/2 [00:08<00:00,  4.07s/it, v_num=0]`Trainer.fit` stopped: `max_steps=60` reached.\n",
            "Epoch 29: 100% 2/2 [00:08<00:00,  4.07s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot16_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot16_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 16,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 120,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot16_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot16_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 8, 1: 8}\n",
            "2024-04-26 19:34:08.818733: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:34:08.818791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:34:08.820337: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:34:09.927052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 16\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 4/4 [00:00<00:00,  5.07it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:08, 12.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:07, 12.46it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.39it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:03, 12.91it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:02, 13.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.59it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.03it/s]\u001b[A\n",
            "{\"AUC\": 0.6554916732751785, \"PR\": 0.09055270969150159, \"micro_f1\": 0.4608610567514677, \"macro_f1\": 0.36390736069708784, \"accuracy\": 0.4608610567514677, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.3368701076320939, \"score_cand\": 0.40174599962925023}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6554916732751785] at step 120.\n",
            "\n",
            "Epoch 29: 100% 4/4 [00:09<00:00,  2.43s/it, v_num=0]`Trainer.fit` stopped: `max_steps=120` reached.\n",
            "Epoch 29: 100% 4/4 [00:09<00:00,  2.43s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot16_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot16_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 16,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 120,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot16_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot16_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 8, 1: 8}\n",
            "2024-04-26 19:34:58.182597: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:34:58.182644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:34:58.184148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:34:59.337263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 16\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 4/4 [00:00<00:00,  5.21it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.72it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.18it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.91it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.26it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.19it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.29it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.34it/s]\u001b[A\n",
            "{\"AUC\": 0.6029260780287475, \"PR\": 0.08069792233171783, \"micro_f1\": 0.47847358121330724, \"macro_f1\": 0.3630865036205407, \"accuracy\": 0.47847358121330724, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.4143170321290974, \"score_cand\": 0.5265817660874816}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6029260780287475] at step 120.\n",
            "\n",
            "Epoch 29: 100% 4/4 [00:08<00:00,  2.18s/it, v_num=0]`Trainer.fit` stopped: `max_steps=120` reached.\n",
            "Epoch 29: 100% 4/4 [00:08<00:00,  2.18s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot16_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot16_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 16,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 120,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot16_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot16_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 8, 1: 8}\n",
            "2024-04-26 19:35:45.499227: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:35:45.499283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:35:45.501269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:35:46.592320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 16\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 4/4 [00:00<00:00,  4.39it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:11,  9.44it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:07, 11.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 13.19it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 14.21it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 14.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 15.69it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 16.13it/s]\u001b[A\n",
            "{\"AUC\": 0.5215122199592668, \"PR\": 0.05425732643408481, \"micro_f1\": 0.449119373776908, \"macro_f1\": 0.3352403948911392, \"accuracy\": 0.449119373776908, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.32024782184518713, \"score_cand\": 0.5539764882067179}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.5215122199592668] at step 120.\n",
            "\n",
            "Epoch 29: 100% 4/4 [00:09<00:00,  2.45s/it, v_num=0]`Trainer.fit` stopped: `max_steps=120` reached.\n",
            "Epoch 29: 100% 4/4 [00:09<00:00,  2.45s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot16_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot16_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 16,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 120,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot16_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot16_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 8, 1: 8}\n",
            "2024-04-26 19:36:34.861507: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:36:34.861566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:36:34.862937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:36:35.966748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 16\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 4/4 [00:00<00:00,  5.06it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 15.27it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.88it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 15.24it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 14.29it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.63it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.06it/s]\u001b[A\n",
            "{\"AUC\": 0.6809626692801141, \"PR\": 0.12913492607286517, \"micro_f1\": 0.8542074363992173, \"macro_f1\": 0.5572219879912187, \"accuracy\": 0.8542074363992173, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.31800267467760057, \"score_cand\": 0.463155339608687}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6809626692801141] at step 120.\n",
            "\n",
            "Epoch 29: 100% 4/4 [00:10<00:00,  2.52s/it, v_num=0]`Trainer.fit` stopped: `max_steps=120` reached.\n",
            "Epoch 29: 100% 4/4 [00:10<00:00,  2.52s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot16_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot16_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 16,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 120,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot16_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot16_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 8, 1: 8}\n",
            "2024-04-26 19:37:24.070446: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:37:24.070884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:37:24.072889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:37:25.469345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 16\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 4/4 [00:00<00:00,  5.16it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.48it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:03, 17.04it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.34it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.52it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.77it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.86it/s]\u001b[A\n",
            "{\"AUC\": 0.48528044069251675, \"PR\": 0.061883137172427674, \"micro_f1\": 0.5968688845401174, \"macro_f1\": 0.43082538051853253, \"accuracy\": 0.5968688845401174, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.43591380819182574, \"score_cand\": 0.952672562720491}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.48528044069251675] at step 120.\n",
            "\n",
            "Epoch 29: 100% 4/4 [00:08<00:00,  2.15s/it, v_num=0]`Trainer.fit` stopped: `max_steps=120` reached.\n",
            "Epoch 29: 100% 4/4 [00:08<00:00,  2.15s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot32_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot32_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 32,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 240,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot32_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot32_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 16, 1: 16}\n",
            "2024-04-26 19:38:14.801598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:38:14.801661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:38:14.803039: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:38:15.889626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 32\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 8/8 [00:01<00:00,  5.24it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.67it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.62it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.29it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 16.72it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.29it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.67it/s]\u001b[A\n",
            "{\"AUC\": 0.7435368754956383, \"PR\": 0.11101916136495436, \"micro_f1\": 0.8493150684931505, \"macro_f1\": 0.561471161883533, \"accuracy\": 0.8493150684931506, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.5804132211464959, \"score_cand\": 1.0039095943921232}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7435368754956383] at step 240.\n",
            "\n",
            "Epoch 29: 100% 8/8 [00:09<00:00,  1.17s/it, v_num=0]`Trainer.fit` stopped: `max_steps=240` reached.\n",
            "Epoch 29: 100% 8/8 [00:09<00:00,  1.17s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot32_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot32_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 32,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 240,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot32_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot32_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 16, 1: 16}\n",
            "2024-04-26 19:39:19.790195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:39:19.790308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:39:19.791779: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:39:20.878192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 32\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 8/8 [00:01<00:00,  4.25it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:06, 16.10it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 17.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:03, 17.47it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.63it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.68it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 18.11it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:06<00:00, 18.40it/s]\u001b[A\n",
            "{\"AUC\": 0.606957991101985, \"PR\": 0.07160409388463648, \"micro_f1\": 0.4726027397260274, \"macro_f1\": 0.3635782821426715, \"accuracy\": 0.4726027397260274, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.7409325168557364, \"score_cand\": 0.802270937805773}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.606957991101985] at step 240.\n",
            "\n",
            "Epoch 29: 100% 8/8 [00:09<00:00,  1.23s/it, v_num=0]`Trainer.fit` stopped: `max_steps=240` reached.\n",
            "Epoch 29: 100% 8/8 [00:09<00:00,  1.23s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot32_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot32_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 32,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 240,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot32_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot32_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 16, 1: 16}\n",
            "2024-04-26 19:40:25.274728: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:40:25.274781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:40:25.276204: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:40:26.403572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 32\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 8/8 [00:01<00:00,  6.24it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:13,  7.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:04<00:10,  8.57it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:06<00:07,  8.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:08<00:04,  9.92it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:09<00:02, 10.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:10<00:00, 11.89it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:10<00:00, 12.31it/s]\u001b[A\n",
            "{\"AUC\": 0.6509292260692464, \"PR\": 0.11187157557960861, \"micro_f1\": 0.7407045009784737, \"macro_f1\": 0.4948285349225446, \"accuracy\": 0.7407045009784736, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.6440774876544154, \"score_cand\": 0.8591498507445572}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6509292260692464] at step 240.\n",
            "\n",
            "Epoch 29: 100% 8/8 [00:12<00:00,  1.56s/it, v_num=0]`Trainer.fit` stopped: `max_steps=240` reached.\n",
            "Epoch 29: 100% 8/8 [00:12<00:00,  1.56s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot32_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot32_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 32,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 240,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot32_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot32_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 16, 1: 16}\n",
            "2024-04-26 19:41:34.872184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:41:34.872236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:41:34.873591: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:41:36.026052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 32\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 8/8 [00:01<00:00,  6.27it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:06, 15.49it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:06, 13.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 13.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:03, 13.21it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:02, 13.81it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.63it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.06it/s]\u001b[A\n",
            "{\"AUC\": 0.7572166785459729, \"PR\": 0.12700724331161142, \"micro_f1\": 0.7729941291585127, \"macro_f1\": 0.525640204865557, \"accuracy\": 0.7729941291585127, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.6082057766251835, \"score_cand\": 0.8640592989389677}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7572166785459729] at step 240.\n",
            "\n",
            "Epoch 29: 100% 8/8 [00:10<00:00,  1.30s/it, v_num=0]`Trainer.fit` stopped: `max_steps=240` reached.\n",
            "Epoch 29: 100% 8/8 [00:10<00:00,  1.30s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot32_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot32_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 32,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 240,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot32_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot32_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 16, 1: 16}\n",
            "2024-04-26 19:42:40.972482: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:42:40.972535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:42:40.974073: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:42:42.066549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 32\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 8/8 [00:01<00:00,  6.30it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.48it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.25it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.28it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 14.90it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:01, 14.16it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.43it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 14.84it/s]\u001b[A\n",
            "{\"AUC\": 0.6177743597081128, \"PR\": 0.07544213093071353, \"micro_f1\": 0.6448140900195695, \"macro_f1\": 0.45513346796401694, \"accuracy\": 0.6448140900195695, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.6573116056139922, \"score_cand\": 0.7316089488288894}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6177743597081128] at step 240.\n",
            "\n",
            "Epoch 29: 100% 8/8 [00:10<00:00,  1.31s/it, v_num=0]`Trainer.fit` stopped: `max_steps=240` reached.\n",
            "Epoch 29: 100% 8/8 [00:10<00:00,  1.31s/it, v_num=0]\n",
            "Start experiment t5_stroke_numshot64_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot64_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 64,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 480,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot64_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot64_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 32, 1: 32}\n",
            "2024-04-26 19:43:46.707322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:43:46.707378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:43:46.709459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:43:47.880677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 64\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 16/16 [00:02<00:00,  5.81it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:11,  9.45it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:07, 12.44it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:04, 13.88it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 14.79it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 15.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 16.13it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 16.54it/s]\u001b[A\n",
            "{\"AUC\": 0.7250198255352894, \"PR\": 0.11239334945662792, \"micro_f1\": 0.6663405088062623, \"macro_f1\": 0.4844911875864415, \"accuracy\": 0.6663405088062623, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.5911468685023239, \"score_cand\": 1.9853074210030692}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7250198255352894] at step 480.\n",
            "\n",
            "Epoch 29: 100% 16/16 [00:11<00:00,  1.40it/s, v_num=0]`Trainer.fit` stopped: `max_steps=480` reached.\n",
            "Epoch 29: 100% 16/16 [00:11<00:00,  1.40it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot64_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot64_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 64,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 480,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot64_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot64_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 32, 1: 32}\n",
            "2024-04-26 19:45:27.720752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:45:27.720814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:45:27.722388: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:45:28.822135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 64\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 16/16 [00:02<00:00,  6.00it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:08, 13.09it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.11it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.52it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 16.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.40it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.80it/s]\u001b[A\n",
            "{\"AUC\": 0.6810831622176592, \"PR\": 0.08219808450219664, \"micro_f1\": 0.6976516634050881, \"macro_f1\": 0.48022180415858107, \"accuracy\": 0.6976516634050881, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.6512960445157703, \"score_cand\": 1.2085184295116804}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6810831622176592] at step 480.\n",
            "\n",
            "Epoch 29: 100% 16/16 [00:10<00:00,  1.55it/s, v_num=0]`Trainer.fit` stopped: `max_steps=480` reached.\n",
            "Epoch 29: 100% 16/16 [00:10<00:00,  1.55it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot64_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot64_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 64,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 480,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot64_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot64_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 32, 1: 32}\n",
            "2024-04-26 19:47:03.652962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:47:03.653013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:47:03.654443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:47:04.749220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 64\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 16/16 [00:02<00:00,  6.61it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:11,  9.19it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:07, 11.40it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:05<00:05, 11.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:04, 11.97it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:08<00:02, 12.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:09<00:00, 13.00it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:09<00:00, 13.41it/s]\u001b[A\n",
            "{\"AUC\": 0.6727978615071284, \"PR\": 0.10381875761188546, \"micro_f1\": 0.7240704500978472, \"macro_f1\": 0.47491200069962175, \"accuracy\": 0.7240704500978473, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.8503497771320978, \"score_cand\": 1.1979016492511427}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6727978615071284] at step 480.\n",
            "\n",
            "Epoch 29: 100% 16/16 [00:12<00:00,  1.24it/s, v_num=0]`Trainer.fit` stopped: `max_steps=480` reached.\n",
            "Epoch 29: 100% 16/16 [00:12<00:00,  1.24it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot64_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot64_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 64,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 480,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot64_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot64_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 32, 1: 32}\n",
            "2024-04-26 19:48:44.158998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:48:44.159053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:48:44.160515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:48:45.290038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 64\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 16/16 [00:02<00:00,  7.15it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:13,  8.13it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:04<00:09,  9.62it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:05<00:05, 11.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:03, 12.58it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:02, 13.46it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.35it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 14.77it/s]\u001b[A\n",
            "{\"AUC\": 0.6816642908054169, \"PR\": 0.10483632133959195, \"micro_f1\": 0.6819960861056752, \"macro_f1\": 0.47577584070782497, \"accuracy\": 0.6819960861056752, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.8657417670621331, \"score_cand\": 1.557129600509972}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6816642908054169] at step 480.\n",
            "\n",
            "Epoch 29: 100% 16/16 [00:11<00:00,  1.41it/s, v_num=0]`Trainer.fit` stopped: `max_steps=480` reached.\n",
            "Epoch 29: 100% 16/16 [00:11<00:00,  1.41it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot64_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot64_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 64,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 480,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot64_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot64_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 32, 1: 32}\n",
            "2024-04-26 19:50:21.442145: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:50:21.442207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:50:21.443621: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:50:22.540758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 64\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 16/16 [00:02<00:00,  6.17it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:08, 13.26it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:06, 14.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 15.36it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 15.97it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 16.25it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 16.86it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.22it/s]\u001b[A\n",
            "{\"AUC\": 0.6919713120618115, \"PR\": 0.10565829298370619, \"micro_f1\": 0.6555772994129159, \"macro_f1\": 0.4725779967159277, \"accuracy\": 0.6555772994129159, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.8135810075674038, \"score_cand\": 1.621915309629795}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6919713120618115] at step 480.\n",
            "\n",
            "Epoch 29: 100% 16/16 [00:10<00:00,  1.51it/s, v_num=0]`Trainer.fit` stopped: `max_steps=480` reached.\n",
            "Epoch 29: 100% 16/16 [00:10<00:00,  1.51it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot128_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot128_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 128,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 960,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot128_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot128_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 64, 1: 64}\n",
            "2024-04-26 19:52:01.155508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:52:01.155562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:52:01.157010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:52:02.577090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 128\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 32/32 [00:04<00:00,  7.49it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:08, 12.31it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:06, 13.63it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.46it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:04, 11.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:08<00:02, 11.24it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:10<00:00, 11.54it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:10<00:00, 11.79it/s]\u001b[A\n",
            "{\"AUC\": 0.6976605868358445, \"PR\": 0.10057364953046269, \"micro_f1\": 0.6829745596868885, \"macro_f1\": 0.48867173988239365, \"accuracy\": 0.6829745596868885, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.7534979504848413, \"score_cand\": 1.7941028180654268}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6976605868358445] at step 960.\n",
            "\n",
            "Epoch 29: 100% 32/32 [00:15<00:00,  2.03it/s, v_num=0]`Trainer.fit` stopped: `max_steps=960` reached.\n",
            "Epoch 29: 100% 32/32 [00:15<00:00,  2.03it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot128_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot128_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 128,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 960,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot128_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot128_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 64, 1: 64}\n",
            "2024-04-26 19:54:51.522339: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:54:51.522391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:54:51.523911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:54:52.803819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 128\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 32/32 [00:05<00:00,  6.31it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 13.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.69it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.61it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.92it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.16it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.69it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 18.02it/s]\u001b[A\n",
            "{\"AUC\": 0.7406528062970568, \"PR\": 0.10302017805021112, \"micro_f1\": 0.6927592954990215, \"macro_f1\": 0.4883221082708301, \"accuracy\": 0.6927592954990215, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.6720409598602474, \"score_cand\": 1.5242005756923132}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7406528062970568] at step 960.\n",
            "\n",
            "Epoch 29: 100% 32/32 [00:12<00:00,  2.47it/s, v_num=0]`Trainer.fit` stopped: `max_steps=960` reached.\n",
            "Epoch 29: 100% 32/32 [00:12<00:00,  2.46it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot128_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot128_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 128,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 960,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot128_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot128_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 64, 1: 64}\n",
            "2024-04-26 19:57:41.723466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 19:57:41.723519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 19:57:41.724926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 19:57:42.897083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 128\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 32/32 [00:04<00:00,  7.48it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 13.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:06, 14.31it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:03, 12.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:02, 12.64it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 13.47it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:09<00:00, 13.88it/s]\u001b[A\n",
            "{\"AUC\": 0.6974414460285132, \"PR\": 0.10980767468370668, \"micro_f1\": 0.6301369863013698, \"macro_f1\": 0.44712482468443193, \"accuracy\": 0.6301369863013698, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.727132819636694, \"score_cand\": 1.6086043266400898}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6974414460285132] at step 960.\n",
            "\n",
            "Epoch 29: 100% 32/32 [00:14<00:00,  2.25it/s, v_num=0]`Trainer.fit` stopped: `max_steps=960` reached.\n",
            "Epoch 29: 100% 32/32 [00:14<00:00,  2.25it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot128_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot128_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 128,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 960,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot128_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot128_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 64, 1: 64}\n",
            "2024-04-26 20:00:35.031151: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:00:35.031197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:00:35.032662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:00:36.233123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 128\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 32/32 [00:04<00:00,  6.68it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:08, 13.18it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.23it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.07it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 16.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.39it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.81it/s]\u001b[A\n",
            "{\"AUC\": 0.7222692444761227, \"PR\": 0.2136428231785851, \"micro_f1\": 0.6966731898238747, \"macro_f1\": 0.4863222475874235, \"accuracy\": 0.6966731898238747, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.8366696419314629, \"score_cand\": 1.880230968945647}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7222692444761227] at step 960.\n",
            "\n",
            "Epoch 29: 100% 32/32 [00:12<00:00,  2.49it/s, v_num=0]`Trainer.fit` stopped: `max_steps=960` reached.\n",
            "Epoch 29: 100% 32/32 [00:12<00:00,  2.49it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot128_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot128_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 128,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 960,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot128_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot128_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 64, 1: 64}\n",
            "2024-04-26 20:03:21.955108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:03:21.955160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:03:21.956590: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:03:23.042781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 128\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 32/32 [00:04<00:00,  7.28it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:10, 10.03it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:06, 12.71it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:04, 14.27it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 15.14it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 15.72it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 16.38it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 16.79it/s]\u001b[A\n",
            "{\"AUC\": 0.7451799255973672, \"PR\": 0.13839095675213456, \"micro_f1\": 0.7064579256360078, \"macro_f1\": 0.49862637362637363, \"accuracy\": 0.7064579256360078, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.9028704059100664, \"score_cand\": 2.633281941049953}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7451799255973672] at step 960.\n",
            "\n",
            "Epoch 29: 100% 32/32 [00:12<00:00,  2.47it/s, v_num=0]`Trainer.fit` stopped: `max_steps=960` reached.\n",
            "Epoch 29: 100% 32/32 [00:12<00:00,  2.47it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot256_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot256_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 256,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 1920,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot256_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot256_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 128, 1: 128}\n",
            "2024-04-26 20:06:03.652629: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:06:03.652682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:06:03.654256: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:06:04.740399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 256\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 64/64 [00:08<00:00,  7.20it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 15.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.45it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.87it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.28it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 15.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 15.29it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.46it/s]\u001b[A\n",
            "{\"AUC\": 0.6950138778747026, \"PR\": 0.09971340225761469, \"micro_f1\": 0.7084148727984344, \"macro_f1\": 0.4931905319772085, \"accuracy\": 0.7084148727984344, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.72474366764733, \"score_cand\": 2.6345960053445774}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.6950138778747026] at step 1920.\n",
            "\n",
            "Epoch 29: 100% 64/64 [00:17<00:00,  3.61it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1920` reached.\n",
            "Epoch 29: 100% 64/64 [00:17<00:00,  3.61it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot256_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot256_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 256,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 1920,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot256_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot256_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 128, 1: 128}\n",
            "2024-04-26 20:10:52.375986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:10:52.376046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:10:52.377912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:10:53.965357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 256\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 64/64 [00:07<00:00,  8.06it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:10,  9.90it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:08, 10.80it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.14it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 13.34it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:01, 14.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.95it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.20it/s]\u001b[A\n",
            "{\"AUC\": 0.7603952772073922, \"PR\": 0.12273203462714669, \"micro_f1\": 0.723091976516634, \"macro_f1\": 0.5088365805736178, \"accuracy\": 0.723091976516634, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.7524964692774584, \"score_cand\": 2.234171413862075}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7603952772073922] at step 1920.\n",
            "\n",
            "Epoch 29: 100% 64/64 [00:16<00:00,  3.78it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1920` reached.\n",
            "Epoch 29: 100% 64/64 [00:16<00:00,  3.78it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot256_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot256_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 256,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 1920,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot256_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot256_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 128, 1: 128}\n",
            "2024-04-26 20:15:37.838416: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:15:37.838489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:15:37.840412: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:15:38.955065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 256\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 64/64 [00:09<00:00,  7.05it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.46it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:03, 17.02it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.39it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.51it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 16.96it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 17.16it/s]\u001b[A\n",
            "{\"AUC\": 0.7669551934826883, \"PR\": 0.17724836477768405, \"micro_f1\": 0.6947162426614482, \"macro_f1\": 0.48080267297133594, \"accuracy\": 0.6947162426614482, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.7216897915953993, \"score_cand\": 1.840916754914823}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7669551934826883] at step 1920.\n",
            "\n",
            "Epoch 29: 100% 64/64 [00:17<00:00,  3.74it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1920` reached.\n",
            "Epoch 29: 100% 64/64 [00:17<00:00,  3.74it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot256_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot256_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 256,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 1920,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot256_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot256_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 128, 1: 128}\n",
            "2024-04-26 20:20:25.446457: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:20:25.446511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:20:25.447977: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:20:26.876215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 256\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 64/64 [00:07<00:00,  8.06it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 13.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:06, 12.71it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.64it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 13.63it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 14.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 15.27it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.73it/s]\u001b[A\n",
            "{\"AUC\": 0.7272362794012829, \"PR\": 0.20957129843856295, \"micro_f1\": 0.6232876712328768, \"macro_f1\": 0.4488127207193578, \"accuracy\": 0.6232876712328768, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.8426308379947789, \"score_cand\": 1.872328696652168}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7272362794012829] at step 1920.\n",
            "\n",
            "Epoch 29: 100% 64/64 [00:16<00:00,  3.82it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1920` reached.\n",
            "Epoch 29: 100% 64/64 [00:16<00:00,  3.82it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot256_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot256_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 256,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 1920,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot256_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot256_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 128, 1: 128}\n",
            "2024-04-26 20:25:10.433973: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:25:10.434019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:25:10.435445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:25:11.498640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 256\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 64/64 [00:08<00:00,  7.15it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:06, 16.24it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 17.40it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:03, 17.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.93it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 18.39it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:06<00:00, 18.77it/s]\u001b[A\n",
            "{\"AUC\": 0.7567606238374588, \"PR\": 0.14230965605086254, \"micro_f1\": 0.6487279843444227, \"macro_f1\": 0.48768353219624244, \"accuracy\": 0.6487279843444227, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 1.0494784739619365, \"score_cand\": 2.307483143069273}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7567606238374588] at step 1920.\n",
            "\n",
            "Epoch 29: 100% 64/64 [00:16<00:00,  3.87it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1920` reached.\n",
            "Epoch 29: 100% 64/64 [00:16<00:00,  3.87it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot512_seed42_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot512_seed42_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 42,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 512,\n",
            "    \"few_shot_random_seed\": 42,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 3840,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot512_seed42_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot512_seed42_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3891, 1: 197} to new {0: 256, 1: 256}\n",
            "2024-04-26 20:29:56.981325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:29:56.981384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:29:56.982983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:29:58.083961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 512\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 128/128 [00:16<00:00,  7.59it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 14.02it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 15.88it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 14.74it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:07<00:01, 14.06it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:08<00:00, 14.57it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.00it/s]\u001b[A\n",
            "{\"AUC\": 0.8025277557494053, \"PR\": 0.15700299997540187, \"micro_f1\": 0.7690802348336595, \"macro_f1\": 0.5536655415408301, \"accuracy\": 0.7690802348336595, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.7373175462398044, \"score_cand\": 3.8108505764119545}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.8025277557494053] at step 3840.\n",
            "\n",
            "Epoch 29: 100% 128/128 [00:25<00:00,  4.93it/s, v_num=0]`Trainer.fit` stopped: `max_steps=3840` reached.\n",
            "Epoch 29: 100% 128/128 [00:25<00:00,  4.93it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot512_seed1024_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot512_seed1024_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1024,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 512,\n",
            "    \"few_shot_random_seed\": 1024,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 3840,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot512_seed1024_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot512_seed1024_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3887, 1: 201} to new {0: 256, 1: 256}\n",
            "2024-04-26 20:39:00.976656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:39:00.976720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:39:00.978810: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:39:02.153336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 512\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 128/128 [00:16<00:00,  7.63it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:02<00:11,  9.38it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:03<00:07, 11.26it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.95it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:05<00:03, 13.98it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:06<00:01, 14.69it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:07<00:00, 15.43it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:08<00:00, 15.81it/s]\u001b[A\n",
            "{\"AUC\": 0.7617428131416837, \"PR\": 0.17443270377404793, \"micro_f1\": 0.7798434442270059, \"macro_f1\": 0.5301955832868535, \"accuracy\": 0.7798434442270059, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.9608175143337063, \"score_cand\": 4.403498784436638}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7617428131416837] at step 3840.\n",
            "\n",
            "Epoch 29: 100% 128/128 [00:25<00:00,  4.96it/s, v_num=0]`Trainer.fit` stopped: `max_steps=3840` reached.\n",
            "Epoch 29: 100% 128/128 [00:25<00:00,  4.96it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot512_seed0_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot512_seed0_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 512,\n",
            "    \"few_shot_random_seed\": 0,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 3840,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot512_seed0_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot512_seed0_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3879, 1: 209} to new {0: 256, 1: 256}\n",
            "2024-04-26 20:48:02.795410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:48:02.795467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:48:02.797036: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:48:03.876536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 512\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 128/128 [00:16<00:00,  7.55it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:06, 15.98it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 15.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:04<00:05, 12.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:06<00:04, 11.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:08<00:02, 11.20it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:10<00:00, 11.80it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:10<00:00, 12.21it/s]\u001b[A\n",
            "{\"AUC\": 0.7414332993890019, \"PR\": 0.09505144072050851, \"micro_f1\": 0.7377690802348336, \"macro_f1\": 0.5009801992464491, \"accuracy\": 0.7377690802348337, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.8454178639354071, \"score_cand\": 4.311143146335729}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7414332993890019] at step 3840.\n",
            "\n",
            "Epoch 29: 100% 128/128 [00:28<00:00,  4.54it/s, v_num=0]`Trainer.fit` stopped: `max_steps=3840` reached.\n",
            "Epoch 29: 100% 128/128 [00:28<00:00,  4.54it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot512_seed1_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot512_seed1_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 1,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 512,\n",
            "    \"few_shot_random_seed\": 1,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 3840,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot512_seed1_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot512_seed1_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3885, 1: 203} to new {0: 256, 1: 256}\n",
            "2024-04-26 20:57:08.364550: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 20:57:08.364618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 20:57:08.366583: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 20:57:09.582646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 512\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 128/128 [00:18<00:00,  6.95it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:06, 15.87it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 17.18it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:03, 17.61it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 17.12it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.39it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.95it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:06<00:00, 18.38it/s]\u001b[A\n",
            "{\"AUC\": 0.789669458303635, \"PR\": 0.14750824748934274, \"micro_f1\": 0.7632093933463796, \"macro_f1\": 0.537442396313364, \"accuracy\": 0.7632093933463796, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 0.988019585609436, \"score_cand\": 4.441930738913803}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.789669458303635] at step 3840.\n",
            "\n",
            "Epoch 29: 100% 128/128 [00:26<00:00,  4.91it/s, v_num=0]`Trainer.fit` stopped: `max_steps=3840` reached.\n",
            "Epoch 29: 100% 128/128 [00:26<00:00,  4.91it/s, v_num=0]\n",
            "Start experiment t5_stroke_numshot512_seed32_ia3_pretrained100k\n",
            "{\n",
            "    \"exp_dir\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k\",\n",
            "    \"exp_name\": \"t5_stroke_numshot512_seed32_ia3_pretrained100k\",\n",
            "    \"allow_skip_exp\": true,\n",
            "    \"seed\": 32,\n",
            "    \"model\": \"EncDec\",\n",
            "    \"max_seq_len\": 1024,\n",
            "    \"origin_model\": \"google/flan-t5-small\",\n",
            "    \"load_weight\": \"pretrained_checkpoints/t5_ia3_finish.pt\",\n",
            "    \"dataset\": \"stroke\",\n",
            "    \"few_shot\": true,\n",
            "    \"num_shot\": 512,\n",
            "    \"few_shot_random_seed\": 32,\n",
            "    \"train_template_idx\": -1,\n",
            "    \"eval_template_idx\": -1,\n",
            "    \"batch_size\": 4,\n",
            "    \"eval_batch_size\": 8,\n",
            "    \"num_workers\": 8,\n",
            "    \"change_hswag_templates\": false,\n",
            "    \"raft_cross_validation\": true,\n",
            "    \"raft_validation_start\": 0,\n",
            "    \"raft_labels_in_input_string\": \"comma\",\n",
            "    \"cleaned_answer_choices_b77\": false,\n",
            "    \"compute_precision\": \"16-true\",\n",
            "    \"compute_strategy\": \"none\",\n",
            "    \"num_steps\": 3840,\n",
            "    \"eval_epoch_interval\": 30,\n",
            "    \"eval_before_training\": false,\n",
            "    \"save_model\": true,\n",
            "    \"save_step_interval\": 20000,\n",
            "    \"mc_loss\": 1,\n",
            "    \"unlikely_loss\": 1,\n",
            "    \"length_norm\": 1,\n",
            "    \"grad_accum_factor\": 1,\n",
            "    \"split_option_at_inference\": false,\n",
            "    \"optimizer\": \"adafactor\",\n",
            "    \"lr\": 0.003,\n",
            "    \"trainable_param_names\": \".*lora_b.*\",\n",
            "    \"scheduler\": \"linear_decay_with_warmup\",\n",
            "    \"warmup_ratio\": 0.06,\n",
            "    \"weight_decay\": 0,\n",
            "    \"scale_parameter\": true,\n",
            "    \"grad_clip_norm\": 1,\n",
            "    \"model_modifier\": \"lora\",\n",
            "    \"prompt_tuning_num_prefix_emb\": 100,\n",
            "    \"prompt_tuning_encoder\": true,\n",
            "    \"prompt_tuning_decoder\": true,\n",
            "    \"lora_rank\": 0,\n",
            "    \"lora_scaling_rank\": 1,\n",
            "    \"lora_init_scale\": 0.0,\n",
            "    \"lora_modules\": \".*SelfAttention|.*EncDecAttention|.*DenseReluDense\",\n",
            "    \"lora_layers\": \"k|v|wi_1.*\",\n",
            "    \"bitfit_modules\": \".*\",\n",
            "    \"bitfit_layers\": \"q|k|v|o|wi_[01]|w_o\",\n",
            "    \"adapter_type\": \"normal\",\n",
            "    \"adapter_non_linearity\": \"relu\",\n",
            "    \"adapter_reduction_factor\": 4,\n",
            "    \"normal_adapter_residual\": true,\n",
            "    \"lowrank_adapter_w_init\": \"glorot-uniform\",\n",
            "    \"lowrank_adapter_rank\": 1,\n",
            "    \"compacter_hypercomplex_division\": 8,\n",
            "    \"compacter_learn_phm\": true,\n",
            "    \"compacter_hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
            "    \"compacter_shared_phm_rule\": false,\n",
            "    \"compacter_factorized_phm\": false,\n",
            "    \"compacter_shared_W_phm\": false,\n",
            "    \"compacter_factorized_phm_rule\": false,\n",
            "    \"compacter_phm_c_init\": \"normal\",\n",
            "    \"compacter_phm_rank\": 1,\n",
            "    \"compacter_phm_init_range\": 0.01,\n",
            "    \"compacter_kronecker_prod\": false,\n",
            "    \"compacter_add_compacter_in_self_attention\": false,\n",
            "    \"compacter_add_compacter_in_cross_attention\": false,\n",
            "    \"intrinsic_projection\": \"fastfood\",\n",
            "    \"intrinsic_said\": true,\n",
            "    \"intrinsic_dim\": 2000,\n",
            "    \"intrinsic_device\": \"cpu\",\n",
            "    \"fishmask_mode\": null,\n",
            "    \"fishmask_path\": null,\n",
            "    \"fishmask_keep_ratio\": 0.05,\n",
            "    \"prefix_tuning_num_input_tokens\": 10,\n",
            "    \"prefix_tuning_num_target_tokens\": 10,\n",
            "    \"prefix_tuning_init_path\": null,\n",
            "    \"prefix_tuning_init_text\": null,\n",
            "    \"prefix_tuning_parameterization\": \"mlp-512\",\n",
            "    \"train_pred_file\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/train_pred.txt\",\n",
            "    \"dev_pred_file\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/dev_pred.txt\",\n",
            "    \"dev_score_file\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/dev_scores.json\",\n",
            "    \"test_pred_file\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/test_pred.txt\",\n",
            "    \"test_score_file\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/test_scores.json\",\n",
            "    \"finish_flag_file\": \"exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/exp_completed.txt\"\n",
            "}\n",
            "Mark experiment t5_stroke_numshot512_seed32_ia3_pretrained100k as claimed\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:root:Tried instantiating `DatasetTemplates` for stroke stroke, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: exp_out/t5_stroke_numshot512_seed32_ia3_pretrained100k/log\n",
            "Via sampling with replacement old label distribution {0: 3897, 1: 191} to new {0: 256, 1: 256}\n",
            "2024-04-26 21:06:07.776299: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 21:06:07.776351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 21:06:07.777837: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 21:06:08.866788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Train size 512\n",
            "Eval size 1022\n",
            "Test size 0\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Get Scheduler\n",
            "get_linear_schedule_with_warmup\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 77.0 M\n",
            "-----------------------------------------------------\n",
            "34.8 K    Trainable params\n",
            "77.0 M    Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.115   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 29: 100% 128/128 [00:17<00:00,  7.35it/s, v_num=0]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/128 [00:00<?, ?it/s]      \u001b[A\n",
            "Validation DataLoader 0:   0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  16% 20/128 [00:01<00:07, 15.17it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 40/128 [00:02<00:05, 16.05it/s]\u001b[A\n",
            "Validation DataLoader 0:  47% 60/128 [00:03<00:04, 16.66it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 80/128 [00:04<00:02, 16.96it/s]\u001b[A\n",
            "Validation DataLoader 0:  78% 100/128 [00:05<00:01, 17.26it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 120/128 [00:06<00:00, 17.73it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 128/128 [00:07<00:00, 18.08it/s]\u001b[A\n",
            "{\"AUC\": 0.7610441407926742, \"PR\": 0.11688828129701974, \"micro_f1\": 0.7465753424657534, \"macro_f1\": 0.5441923257732172, \"accuracy\": 0.7465753424657534, \"num\": 1022, \"num_steps\": -1, \"score_gt\": 1.1492727202911899, \"score_cand\": 4.365946157337868}\n",
            "\n",
            "Stored new best metric ['AUC'] with values [0.7610441407926742] at step 3840.\n",
            "\n",
            "Epoch 29: 100% 128/128 [00:25<00:00,  5.10it/s, v_num=0]`Trainer.fit` stopped: `max_steps=3840` reached.\n",
            "Epoch 29: 100% 128/128 [00:25<00:00,  5.10it/s, v_num=0]\n"
          ]
        }
      ],
      "source": [
        "# Run Training\n",
        "# !chmod +rwx /content/drive/MyDrive/Colab\\ Notebooks/TabLLM/bin/test.sh\n",
        "# !bash ./bin/test.sh\n",
        "!chmod +rwx /content/drive/MyDrive/Colab\\ Notebooks/TabLLM/bin/few-shot-pretrained-100k.sh\n",
        "!bash ./bin/few-shot-pretrained-100k.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Results\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot4_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot8_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot16_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot32_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot64_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot128_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot256_* -d stroke\n",
        "!python src/scripts/get_result_table.py -e t5_stroke_numshot512_* -d stroke"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv1o8yOSzn91",
        "outputId": "568b0c9a-a3da-4081-dbb1-7a0a165e277d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot4_*\n",
            "stroke: 60.23 (6.44)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot8_*\n",
            "stroke: 60.44 (5.24)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot16_*\n",
            "stroke: 58.92 (7.53)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot32_*\n",
            "stroke: 67.53 (6.32)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot64_*\n",
            "stroke: 69.05 (1.83)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot128_*\n",
            "stroke: 72.06 (2.04)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot256_*\n",
            "stroke: 74.13 (2.68)\n",
            "Save result to exp_out/summary.csv\n",
            "================================================================================\n",
            "Find 5 experiments fit into t5_stroke_numshot512_*\n",
            "stroke: 77.13 (2.19)\n",
            "Save result to exp_out/summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRzxFs5bUXM1"
      },
      "outputs": [],
      "source": [
        "!dmesg"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}